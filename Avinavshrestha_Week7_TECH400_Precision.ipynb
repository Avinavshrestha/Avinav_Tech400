{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLQHsnmlNKnb4ifc9O7PWJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avinavshrestha/Avinav_Tech400/blob/main/Avinavshrestha_Week7_TECH400_Precision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nI230QqMKxk",
        "outputId": "d04c7a7b-30df-4fa2-996e-8be642b5cf91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queries: ['shooting']\n",
            "Number of Relevant Documents (1): 3\n",
            "Number of Non-Relevant Documents (0): 35\n",
            "Logistic Regression Precision: 0.0000\n",
            "Logistic Regression Recall: 0.0000\n",
            "Logistic Regression Accuracy: 0.8750\n",
            "Results saved to result_Avinav.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define function to load documents\n",
        "def load_documents(directory):\n",
        "    documents = {}\n",
        "    filenames = []\n",
        "    for doc_id, filename in enumerate(os.listdir(directory), start=1):\n",
        "        filepath = os.path.join(directory, filename)\n",
        "        if os.path.isfile(filepath):\n",
        "            with open(filepath, 'r', encoding='utf-8') as file:\n",
        "                documents[doc_id] = file.read()\n",
        "                filenames.append(filename)\n",
        "    return documents, filenames\n",
        "\n",
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "# Load documents from the specified directory\n",
        "docs, filenames = load_documents('/content/directory')\n",
        "\n",
        "# Tokenize all documents\n",
        "tokenized_docs = [tokenize(doc) for doc in docs.values()]\n",
        "\n",
        "# Define your queries\n",
        "queries = ['shooting']\n",
        "tokenized_queries = [tokenize(query) for query in queries]\n",
        "\n",
        "print(\"Queries:\", queries)\n",
        "\n",
        "# Creating a sorted vocabulary from all documents\n",
        "vocab = sorted(set([word for doc in tokenized_docs for word in doc]))\n",
        "\n",
        "def term_frequency(term, document):\n",
        "    return document.count(term) / len(document) if len(document) > 0 else 0\n",
        "\n",
        "def inverse_document_frequency(term, docs):\n",
        "    num_docs_containing_term = sum(1 for doc in docs if term in doc)\n",
        "    return math.log(len(docs) / (1 + num_docs_containing_term)) if num_docs_containing_term > 0 else 0\n",
        "\n",
        "def compute_tfidf_vector(document, vocab, docs):\n",
        "    tfidf_vector = []\n",
        "    for term in vocab:\n",
        "        tf = term_frequency(term, document)\n",
        "        idf = inverse_document_frequency(term, docs)\n",
        "        tfidf_vector.append(tf * idf)\n",
        "    return tfidf_vector\n",
        "\n",
        "# Compute TF-IDF vectors for all documents\n",
        "tfidf_docs = [compute_tfidf_vector(doc, vocab, tokenized_docs) for doc in tokenized_docs]\n",
        "\n",
        "# Compute TF-IDF vectors for all queries\n",
        "tfidf_queries = [compute_tfidf_vector(query, vocab, tokenized_docs) for query in tokenized_queries]\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    magnitude = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
        "    if magnitude == 0:\n",
        "        return 0\n",
        "    return dot_product / magnitude\n",
        "\n",
        "# Compute cosine similarities between queries and documents\n",
        "similarities = []\n",
        "for query_vector in tfidf_queries:\n",
        "    doc_similarities = [cosine_similarity(query_vector, doc_vector) for doc_vector in tfidf_docs]\n",
        "    similarities.append(doc_similarities)\n",
        "\n",
        "def rank_documents(similarities):\n",
        "    ranked_docs = sorted(enumerate(similarities), key=lambda x: x[1], reverse=True)\n",
        "    return [(filenames[i], score) for i, score in ranked_docs]\n",
        "\n",
        "# Initialize labels\n",
        "labels = np.zeros(len(tfidf_docs), dtype=int)\n",
        "\n",
        "# Labeling: For each query, mark top N documents as relevant\n",
        "N_per_query = max(2, int(0.1 * len(tfidf_docs)))  # Label top 10% or at least 2 documents per query\n",
        "\n",
        "for query in queries:\n",
        "    query_vec = compute_tfidf_vector(tokenize(query), vocab, tokenized_docs)\n",
        "    # Compute cosine similarities\n",
        "    similarities_query = [cosine_similarity(query_vec, doc_vector) for doc_vector in tfidf_docs]\n",
        "    # Get indices of top N similarities\n",
        "    top_n_indices = np.argsort(similarities_query)[-N_per_query:]\n",
        "    # Label these documents as relevant\n",
        "    for idx in top_n_indices:\n",
        "        labels[idx] = 1\n",
        "\n",
        "# Check label distribution\n",
        "num_relevant = sum(labels)\n",
        "num_non_relevant = len(labels) - num_relevant\n",
        "print(f\"Number of Relevant Documents (1): {num_relevant}\")\n",
        "print(f\"Number of Non-Relevant Documents (0): {num_non_relevant}\")\n",
        "\n",
        "# Ensure both classes have at least two samples\n",
        "if num_relevant < 2 or num_non_relevant < 2:A\n",
        "    raise ValueError(\"Insufficient samples in one of the classes after labeling. Adjust N or labeling strategy.\")\n",
        "\n",
        "# Convert TF-IDF lists to numpy arrays\n",
        "X = np.array(tfidf_docs)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "# Using stratify to maintain class distribution\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Initialize and train Logistic Regression model\n",
        "clf = LogisticRegression(max_iter=1000, solver='liblinear', class_weight='balanced')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate Precision, Recall, and Accuracy\n",
        "precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print logistic regression results\n",
        "print(f\"Logistic Regression Precision: {precision:.4f}\")\n",
        "print(f\"Logistic Regression Recall: {recall:.4f}\")\n",
        "print(f\"Logistic Regression Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Save the results (precision, recall, accuracy) to the result_Avinav.txt file\n",
        "with open(\"result_Avinav.txt\", 'w') as file:\n",
        "    file.write(f\"Logistic Regression Precision: {precision:.4f}\\n\")\n",
        "    file.write(f\"Logistic Regression Recall: {recall:.4f}\\n\")\n",
        "    file.write(f\"Logistic Regression Accuracy: {accuracy:.4f}\\n\")\n",
        "\n",
        "print(\"Results saved to result_Avinav.txt\")\n",
        "\n"
      ]
    }
  ]
}